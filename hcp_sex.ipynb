{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib import models_siamese, graph, abide_utils\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random state is 665\n"
     ]
    }
   ],
   "source": [
    "rs = 665\n",
    "\n",
    "print(\"Random state is %d\" % rs)\n",
    "prng = np.random.RandomState(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 268, 268)\n"
     ]
    }
   ],
   "source": [
    "# get the subjects feature matrix\n",
    "# import h5py\n",
    "subset = range(500)\n",
    "all_mats = scipy.io.loadmat('/ysm-gpfs/home/sg972/project/data/all_mats_rest.mat')\n",
    "all_mats = all_mats['all_mats']\n",
    "all_mats = np.array(all_mats)\n",
    "all_mats = np.transpose(all_mats[:, :, subset], (2,0,1)) # get the rest data\n",
    "print(np.shape(all_mats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1)\n"
     ]
    }
   ],
   "source": [
    "# get the subjects label\n",
    "num_train = np.shape(all_mats)[0]\n",
    "label = scipy.io.loadmat('/ysm-gpfs/home/sg972/project/data/male_index.mat')\n",
    "label = label['male_index']\n",
    "all_labels = np.zeros((515, 1))\n",
    "all_labels[label] = 1\n",
    "all_labels = all_labels[subset]\n",
    "print(np.shape(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(site, train_perc):\n",
    "    \"\"\" Split data into training and test indices \"\"\"\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    for s in np.unique(site):\n",
    "        # Make sure each site is represented in both training and test sets\n",
    "        id_in_site = np.argwhere(site == s)[:,0]\n",
    "        num_nodes = len(id_in_site)\n",
    "        train_num = int(train_perc * num_nodes)\n",
    "        prng.shuffle(id_in_site)\n",
    "        train_indices.extend(id_in_site[:train_num])\n",
    "        test_indices.extend(id_in_site[train_num:])\n",
    "\n",
    "    # print(\"Number of labeled samples %d\" % len(train_indices))\n",
    "\n",
    "    return train_indices, test_indices\n",
    "\n",
    "\n",
    "def prepare_pairs(X, y, indices):\n",
    "    \"\"\" Prepare the graph pairs before feeding them to the network \"\"\"\n",
    "    N, M, F = X.shape\n",
    "    n_pairs = int(len(indices) * (len(indices) - 1) / 2)\n",
    "    triu_pairs = np.triu_indices(len(indices), 1)\n",
    "    \n",
    "    site_pairs = np.ones(int(n_pairs))\n",
    "    \n",
    "    X_pairs = np.ones((n_pairs, M, F, 2))\n",
    "    X_pairs[:, :, :, 0] = X[indices][triu_pairs[0]]\n",
    "    X_pairs[:, :, :, 1] = X[indices][triu_pairs[1]]\n",
    "\n",
    "#     site_pairs = np.ones(int(n_pairs))\n",
    "#     print(site[indices][triu_pairs[0]] != site[indices][triu_pairs[1]])\n",
    "#     site_pairs[site[indices][triu_pairs[0]] != site[indices][triu_pairs[1]]] = 0\n",
    "\n",
    "    y_pairs = np.ones(int(n_pairs))\n",
    "    y_index = y[triu_pairs[0]] != y[triu_pairs[1]]\n",
    "    y_pairs[y_index.flatten()] = 0  # -1\n",
    "\n",
    "    print(n_pairs)\n",
    "\n",
    "    return X_pairs, y_pairs, site_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The num of training samples are: \n",
      "390\n"
     ]
    }
   ],
   "source": [
    "# training_num = int(num_train*0.8)\n",
    "training_num = int(num_train*0.13)*6\n",
    "site = np.ones(num_train)\n",
    "tr_idx, test_idx = split_data(site, 0.7)\n",
    "prng.shuffle(test_idx)\n",
    "subs_to_add = training_num - len(tr_idx)  # subjects that need to be moved from testing to training set\n",
    "tr_idx.extend(test_idx[:subs_to_add])\n",
    "test_idx = test_idx[subs_to_add:]\n",
    "print(\"The num of training samples are: \")\n",
    "print(len(tr_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12675, 2)\n"
     ]
    }
   ],
   "source": [
    "# Compute all the combo pairs\n",
    "all_combs = []\n",
    "tr_mat = np.array(tr_idx).reshape([int(training_num / 6), 6])\n",
    "for i in range(3):\n",
    "    x1 = tr_mat[:, i * 2].flatten()\n",
    "    x2 = tr_mat[:, i * 2 + 1].flatten()\n",
    "    combs = np.transpose([np.tile(x1, len(x2)), np.repeat(x2, len(x1))])\n",
    "    all_combs.append(combs)\n",
    "\n",
    "all_combs = np.vstack(all_combs)\n",
    "\n",
    "print(all_combs.shape) #(training_num/6)*(training_num/6)*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12675, 268, 268, 2)\n"
     ]
    }
   ],
   "source": [
    "n, m, f = all_mats.shape\n",
    "X_train = np.ones((all_combs.shape[0], m, f, 2), dtype=np.float32)\n",
    "y_train = np.ones(all_combs.shape[0], dtype=np.int32)\n",
    "\n",
    "for i in range(all_combs.shape[0]):\n",
    "    X_train[i, :, :, 0] = all_mats[all_combs[i, 0], :, :]\n",
    "    X_train[i, :, :, 1] = all_mats[all_combs[i, 1], :, :]\n",
    "    if all_labels[all_combs[i, 0]] != all_labels[all_combs[i, 1]]:\n",
    "        y_train[i] = 0  # -1\n",
    "        \n",
    "print(np.shape(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(268, 268)\n"
     ]
    }
   ],
   "source": [
    "# Get the graph structure\n",
    "\n",
    "coords_file = '/ysm-gpfs/home/sg972/project/code/gcn_metric_learning-master/shen_coords.csv'\n",
    "coords = np.loadtxt(coords_file, delimiter=',')\n",
    "\n",
    "dist, idx = graph.distance_scipy_spatial(coords, k=10, metric='euclidean')\n",
    "A = graph.adjacency(dist, idx).astype(np.float32)\n",
    "\n",
    "graphs = []\n",
    "for i in range(3):\n",
    "    graphs.append(A)\n",
    "\n",
    "# Calculate Laplacians\n",
    "L = [graph.laplacian(A, normalized=True) for A in graphs]\n",
    "print(np.shape(L[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # print(type(A))\n",
    "# temp = np.abs(np.squeeze(all_mats[1,:,:]))\n",
    "# graphs = []\n",
    "# for i in range(3):\n",
    "#     graphs.append(scipy.sparse.csr.csr_matrix(temp).astype(np.float32))\n",
    "# L = [graph.laplacian(B, normalized=True) for B in graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5995\n"
     ]
    }
   ],
   "source": [
    "# all_mats = []\n",
    "X_test, y_test, site_test = prepare_pairs(all_mats, all_labels, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n, m, f, _ = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graph Conv-net\n",
    "features = 20\n",
    "K = 3\n",
    "params['num_epochs']     = 50\n",
    "params['batch_size']     = 10\n",
    "params['eval_frequency'] = X_train.shape[0] / (params['batch_size'] * 2)\n",
    "# params['eval_frequency'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building blocks.\n",
    "params['filter']         = 'chebyshev5'\n",
    "params['brelu']          = 'b2relu'\n",
    "params['pool']           = 'apool1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Architecture.\n",
    "params['F']              = [features, features]   # Number of graph convolutional filters.\n",
    "params['K']              = [K, K]   # Polynomial orders.\n",
    "params['p']              = [1, 1]     # Pooling sizes.\n",
    "params['M']              = [1]    # Output dimensionality of fully connected layers.\n",
    "params['input_features'] = f\n",
    "params['lamda']          = 0.35\n",
    "params['mu']             = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimization.\n",
    "params['regularization'] = 5e-3\n",
    "params['dropout']        = 0.8 #0.5\n",
    "params['learning_rate']  = 1e-3\n",
    "params['decay_rate']     = 0.95\n",
    "params['momentum']       = 0\n",
    "params['decay_steps']    = X_train.shape[0] / params['batch_size']\n",
    "\n",
    "params['dir_name']       = 'siamese_' + time.strftime(\"%Y_%m_%d_%H_%M\") + '_feat' + str(params['F'][0]) + '_' + \\\n",
    "                           str(params['F'][1]) + '_K' + str(K) + '_state' + str(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/ysm-gpfs/project/sg972/code/gcn_metric_learning-master/logs/siamese_2017_11_23_01_46_feat1_1_K3_state665'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-3f82b45b4ad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__file__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlog_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dir_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(log_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dlnn/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/ysm-gpfs/project/sg972/code/gcn_metric_learning-master/logs/siamese_2017_11_23_01_46_feat1_1_K3_state665'"
     ]
    }
   ],
   "source": [
    "# Save logs to folder\n",
    "path = os.path.dirname(os.path.abspath('__file__'))\n",
    "log_path = os.path.join(path, 'logs', params['dir_name'])\n",
    "os.makedirs(log_path)\n",
    "# print(log_path)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN architecture\n",
      "  input: M_0 = 268\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 268 * 20 / 1 = 5360\n",
      "    weights: F_0 * F_1 * K_1 = 268 * 20 * 3 = 16080\n",
      "    biases: M_1 * F_1 = 268 * 20 = 5360\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 268 * 20 / 1 = 5360\n",
      "    weights: F_1 * F_2 * K_2 = 20 * 20 * 3 = 1200\n",
      "    biases: M_2 * F_2 = 268 * 20 = 5360\n",
      "  layer 3: logits (softmax)\n",
      "    representation: M_3 = 1\n",
      "    weights: M_2 * M_3 = 5360 * 1 = 5360\n",
      "    biases: M_3 = 1\n",
      "(10, 268, 268, 2)\n",
      "step 2535 / 63375 (epoch 2.00 / 50):\n",
      "  learning_rate = 9.50e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 0.55, loss: 2.1180e-01\n",
      "  validation samples: 5995, AUC : 0.51, loss: 2.1205e-01\n",
      "  time: 44s (wall 41s)\n",
      "step 5070 / 63375 (epoch 4.00 / 50):\n",
      "  learning_rate = 8.57e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 0.55, loss: 2.1024e-01\n",
      "  validation samples: 5995, AUC : 0.51, loss: 2.1042e-01\n",
      "  time: 88s (wall 82s)\n",
      "step 7605 / 63375 (epoch 6.00 / 50):\n",
      "  learning_rate = 7.74e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 0.54, loss: 2.0969e-01\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.1085e-01\n",
      "  time: 131s (wall 123s)\n",
      "step 10140 / 63375 (epoch 8.00 / 50):\n",
      "  learning_rate = 6.98e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 0.79, loss: 1.5442e-01\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.2543e-01\n",
      "  time: 175s (wall 164s)\n",
      "step 12675 / 63375 (epoch 10.00 / 50):\n",
      "  learning_rate = 6.30e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 1.00, loss: 7.5257e-03\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.5534e-01\n",
      "  time: 219s (wall 205s)\n",
      "step 15210 / 63375 (epoch 12.00 / 50):\n",
      "  learning_rate = 5.69e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 1.00, loss: 5.0482e-03\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.4951e-01\n",
      "  time: 263s (wall 246s)\n",
      "step 17745 / 63375 (epoch 14.00 / 50):\n",
      "  learning_rate = 5.13e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 1.00, loss: 4.0758e-03\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.4494e-01\n",
      "  time: 306s (wall 287s)\n",
      "step 20280 / 63375 (epoch 16.00 / 50):\n",
      "  learning_rate = 4.63e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 1.00, loss: 1.0345e-02\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.3879e-01\n",
      "  time: 350s (wall 328s)\n",
      "step 22815 / 63375 (epoch 18.00 / 50):\n",
      "  learning_rate = 4.18e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 1.00, loss: 3.5549e-03\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.3792e-01\n",
      "  time: 394s (wall 369s)\n",
      "step 25350 / 63375 (epoch 20.00 / 50):\n",
      "  learning_rate = 3.77e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 1.00, loss: 2.0100e-03\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.4224e-01\n",
      "  time: 438s (wall 410s)\n",
      "step 27885 / 63375 (epoch 22.00 / 50):\n",
      "  learning_rate = 3.41e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 1.00, loss: 1.8390e-03\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.4354e-01\n",
      "  time: 481s (wall 451s)\n",
      "step 30420 / 63375 (epoch 24.00 / 50):\n",
      "  learning_rate = 3.07e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 1.00, loss: 2.2649e-03\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.3630e-01\n",
      "  time: 525s (wall 492s)\n",
      "step 32955 / 63375 (epoch 26.00 / 50):\n",
      "  learning_rate = 2.77e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 1.00, loss: 1.2653e-03\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.3828e-01\n",
      "  time: 568s (wall 532s)\n",
      "step 35490 / 63375 (epoch 28.00 / 50):\n",
      "  learning_rate = 2.50e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 1.00, loss: 1.3796e-03\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.3806e-01\n",
      "  time: 613s (wall 574s)\n",
      "step 38025 / 63375 (epoch 30.00 / 50):\n",
      "  learning_rate = 2.26e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 1.00, loss: 1.0033e-03\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.3626e-01\n",
      "  time: 656s (wall 615s)\n",
      "step 40560 / 63375 (epoch 32.00 / 50):\n",
      "  learning_rate = 2.04e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 1.00, loss: 1.0673e-03\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.3705e-01\n",
      "  time: 700s (wall 656s)\n",
      "step 43095 / 63375 (epoch 34.00 / 50):\n",
      "  learning_rate = 1.84e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 1.00, loss: 9.6545e-04\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.3924e-01\n",
      "  time: 744s (wall 696s)\n",
      "step 45630 / 63375 (epoch 36.00 / 50):\n",
      "  learning_rate = 1.66e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 1.00, loss: 1.0046e-03\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.3675e-01\n",
      "  time: 788s (wall 738s)\n",
      "step 48165 / 63375 (epoch 38.00 / 50):\n",
      "  learning_rate = 1.50e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 1.00, loss: 1.0418e-03\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.3721e-01\n",
      "  time: 831s (wall 779s)\n",
      "step 50700 / 63375 (epoch 40.00 / 50):\n",
      "  learning_rate = 1.35e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 1.00, loss: 1.0333e-03\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.4096e-01\n",
      "  time: 875s (wall 820s)\n",
      "step 53235 / 63375 (epoch 42.00 / 50):\n",
      "  learning_rate = 1.22e-04, loss_average = nan\n",
      "  training samples: 12675, AUC : 1.00, loss: 1.0233e-03\n",
      "  validation samples: 5995, AUC : 0.50, loss: 2.3767e-01\n",
      "  time: 919s (wall 861s)\n",
      "step 55770 / 63375 (epoch 44.00 / 50):\n",
      "  learning_rate = 1.10e-04, loss_average = nan\n"
     ]
    }
   ],
   "source": [
    "# Run model\n",
    "site_train = np.ones(all_combs.shape[0], dtype=np.int32)\n",
    "\n",
    "model = models_siamese.siamese_cgcnn_cor(L, **params)\n",
    "accuracy, loss, t_step, scores_summary = model.fit(X_train, y_train, site_train, X_test, y_test, site_test)\n",
    "print('Time per step: {:.2f} ms'.format(t_step*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
