{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from lib import models_siamese, graph, abide_utils\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random state is 665\n"
     ]
    }
   ],
   "source": [
    "rs = 665\n",
    "\n",
    "print(\"Random state is %d\" % rs)\n",
    "prng = np.random.RandomState(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 268, 268)\n"
     ]
    }
   ],
   "source": [
    "# get the subjects feature matrix\n",
    "import h5py\n",
    "subset = range(200)\n",
    "network = h5py.File('/Users/siyuangao/Working_Space/fmri/data/HCP515/all_mats.mat', 'r')\n",
    "network_data = network['all_mats']\n",
    "all_mats = np.array(network_data)\n",
    "all_mats = all_mats[2, subset, :, :] # get the rest data\n",
    "print(np.shape(all_mats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1)\n"
     ]
    }
   ],
   "source": [
    "# get the subjects label\n",
    "n = np.shape(all_mats)[0]\n",
    "label = scipy.io.loadmat('/Users/siyuangao/Working_Space/fmri/data/HCP515/male_index.mat')\n",
    "label = label['male_index']\n",
    "all_labels = np.zeros((515, 1))\n",
    "all_labels[label] = 1\n",
    "all_labels = all_labels[subset]\n",
    "print(np.shape(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(site, train_perc):\n",
    "    \"\"\" Split data into training and test indices \"\"\"\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    for s in np.unique(site):\n",
    "        # Make sure each site is represented in both training and test sets\n",
    "        id_in_site = np.argwhere(site == s)[:,0]\n",
    "        num_nodes = len(id_in_site)\n",
    "        train_num = int(train_perc * num_nodes)\n",
    "        prng.shuffle(id_in_site)\n",
    "        train_indices.extend(id_in_site[:train_num])\n",
    "        test_indices.extend(id_in_site[train_num:])\n",
    "\n",
    "    # print(\"Number of labeled samples %d\" % len(train_indices))\n",
    "\n",
    "    return train_indices, test_indices\n",
    "\n",
    "\n",
    "def prepare_pairs(X, y, indices):\n",
    "    \"\"\" Prepare the graph pairs before feeding them to the network \"\"\"\n",
    "    N, M, F = X.shape\n",
    "    n_pairs = int(len(indices) * (len(indices) - 1) / 2)\n",
    "    triu_pairs = np.triu_indices(len(indices), 1)\n",
    "    \n",
    "    site_pairs = np.ones(int(n_pairs))\n",
    "    \n",
    "    X_pairs = np.ones((n_pairs, M, F, 2))\n",
    "    X_pairs[:, :, :, 0] = X[indices][triu_pairs[0]]\n",
    "    X_pairs[:, :, :, 1] = X[indices][triu_pairs[1]]\n",
    "\n",
    "#     site_pairs = np.ones(int(n_pairs))\n",
    "#     print(site[indices][triu_pairs[0]] != site[indices][triu_pairs[1]])\n",
    "#     site_pairs[site[indices][triu_pairs[0]] != site[indices][triu_pairs[1]]] = 0\n",
    "\n",
    "    y_pairs = np.ones(int(n_pairs))\n",
    "    y_index = y[triu_pairs[0]] != y[triu_pairs[1]]\n",
    "    y_pairs[y_index.flatten()] = 0  # -1\n",
    "\n",
    "    print(n_pairs)\n",
    "\n",
    "    return X_pairs, y_pairs, site_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test indices are the following: \n",
      "120\n"
     ]
    }
   ],
   "source": [
    "training_num = int(n*0.6)\n",
    "site = np.ones(n)\n",
    "tr_idx, test_idx = split_data(site, 0.6)\n",
    "\n",
    "# prng.shuffle(test_idx)\n",
    "# subs_to_add = training_num - len(tr_idx)  # subjects that need to be moved from testing to training set\n",
    "# tr_idx.extend(test_idx[:subs_to_add])\n",
    "# test_idx = test_idx[subs_to_add:]\n",
    "print(\"The test indices are the following: \")\n",
    "print(len(tr_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 2)\n"
     ]
    }
   ],
   "source": [
    "# Compute all the combo pairs\n",
    "all_combs = []\n",
    "tr_mat = np.array(tr_idx).reshape([int(training_num / 6), 6])\n",
    "for i in range(3):\n",
    "    x1 = tr_mat[:, i * 2].flatten()\n",
    "    x2 = tr_mat[:, i * 2 + 1].flatten()\n",
    "    combs = np.transpose([np.tile(x1, len(x2)), np.repeat(x2, len(x1))])\n",
    "    all_combs.append(combs)\n",
    "\n",
    "all_combs = np.vstack(all_combs)\n",
    "\n",
    "print(all_combs.shape) #(training_num/6)*(training_num/6)*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 268, 268, 2)\n"
     ]
    }
   ],
   "source": [
    "n, m, f = all_mats.shape\n",
    "X_train = np.ones((all_combs.shape[0], m, f, 2), dtype=np.float32)\n",
    "y_train = np.ones(all_combs.shape[0], dtype=np.int32)\n",
    "\n",
    "for i in range(all_combs.shape[0]):\n",
    "    X_train[i, :, :, 0] = all_mats[all_combs[i, 0], :, :]\n",
    "    X_train[i, :, :, 1] = all_mats[all_combs[i, 1], :, :]\n",
    "    if all_labels[all_combs[i, 0]] != all_labels[all_combs[i, 1]]:\n",
    "        y_train[i] = 0  # -1\n",
    "        \n",
    "print(np.shape(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(268, 268)\n"
     ]
    }
   ],
   "source": [
    "# Get the graph structure\n",
    "\n",
    "coords_file = '/Users/siyuangao/Working_Space/fmri/data/shen_coords.csv'\n",
    "coords = np.loadtxt(coords_file, delimiter=',')\n",
    "\n",
    "dist, idx = graph.distance_scipy_spatial(coords, k=10, metric='euclidean')\n",
    "A = graph.adjacency(dist, idx).astype(np.float32)\n",
    "\n",
    "graphs = []\n",
    "for i in range(3):\n",
    "    graphs.append(A)\n",
    "\n",
    "# Calculate Laplacians\n",
    "L = [graph.laplacian(A, normalized=True) for A in graphs]\n",
    "print(np.shape(L[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # print(type(A))\n",
    "# temp = np.abs(np.squeeze(all_mats[1,:,:]))\n",
    "# graphs = []\n",
    "# for i in range(3):\n",
    "#     graphs.append(scipy.sparse.csr.csr_matrix(temp).astype(np.float32))\n",
    "# L = [graph.laplacian(B, normalized=True) for B in graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3160\n"
     ]
    }
   ],
   "source": [
    "# all_mats = []\n",
    "X_test, y_test, site_test = prepare_pairs(all_mats, all_labels, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n, m, f, _ = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graph Conv-net\n",
    "features = 64\n",
    "K = 3\n",
    "params = dict()\n",
    "params['num_epochs']     = 10\n",
    "params['batch_size']     = 10\n",
    "# params['eval_frequency'] = X_train.shape[0] / (params['batch_size'] * 2)\n",
    "params['eval_frequency'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building blocks.\n",
    "params['filter']         = 'chebyshev5'\n",
    "params['brelu']          = 'b2relu'\n",
    "params['pool']           = 'apool1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Architecture.\n",
    "params['F']              = [features, features]   # Number of graph convolutional filters.\n",
    "params['K']              = [K, K]   # Polynomial orders.\n",
    "params['p']              = [1, 1]     # Pooling sizes.\n",
    "params['M']              = [1]    # Output dimensionality of fully connected layers.\n",
    "params['input_features'] = f\n",
    "params['lamda']          = 0.35\n",
    "params['mu']             = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimization.\n",
    "params['regularization'] = 5e-3\n",
    "params['dropout']        = 0.8\n",
    "params['learning_rate']  = 5e-3\n",
    "params['decay_rate']     = 0.95\n",
    "params['momentum']       = 0\n",
    "params['decay_steps']    = X_train.shape[0] / params['batch_size']\n",
    "\n",
    "params['dir_name']       = 'siamese_' + time.strftime(\"%Y_%m_%d_%H_%M\") + '_feat' + str(params['F'][0]) + '_' + \\\n",
    "                           str(params['F'][1]) + '_K' + str(K) + '_state' + str(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/Users/siyuangao/Working_Space/fmri/gcn_metric_learning-master/logs/siamese_2017_11_15_12_05_feat64_64_K3_state665'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-3f82b45b4ad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__file__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlog_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dir_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(log_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/Users/siyuangao/Working_Space/fmri/gcn_metric_learning-master/logs/siamese_2017_11_15_12_05_feat64_64_K3_state665'"
     ]
    }
   ],
   "source": [
    "# Save logs to folder\n",
    "path = os.path.dirname(os.path.abspath('__file__'))\n",
    "log_path = os.path.join(path, 'logs', params['dir_name'])\n",
    "os.makedirs(log_path)\n",
    "# print(log_path)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN architecture\n",
      "  input: M_0 = 268\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 268 * 64 / 1 = 17152\n",
      "    weights: F_0 * F_1 * K_1 = 268 * 64 * 3 = 51456\n",
      "    biases: M_1 * F_1 = 268 * 64 = 17152\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 268 * 64 / 1 = 17152\n",
      "    weights: F_1 * F_2 * K_2 = 64 * 64 * 3 = 12288\n",
      "    biases: M_2 * F_2 = 268 * 64 = 17152\n",
      "  layer 3: logits (softmax)\n",
      "    representation: M_3 = 1\n",
      "    weights: M_2 * M_3 = 17152 * 1 = 17152\n",
      "    biases: M_3 = 1\n",
      "(10, 268, 268, 2)\n",
      "step 50 / 1200 (epoch 0.42 / 10):\n",
      "  learning_rate = 5.00e-03, loss_average = 2.28e-01\n",
      "  training samples: 1200, AUC : 0.54, loss: 2.1374e-01\n",
      "  validation samples: 3160, AUC : 0.50, loss: 2.1470e-01\n",
      "  time: 249s (wall 45s)\n",
      "step 100 / 1200 (epoch 0.83 / 10):\n",
      "  learning_rate = 5.00e-03, loss_average = 2.15e-01\n",
      "  training samples: 1200, AUC : 0.55, loss: 2.1413e-01\n",
      "  validation samples: 3160, AUC : 0.49, loss: 2.1462e-01\n",
      "  time: 501s (wall 90s)\n",
      "step 150 / 1200 (epoch 1.25 / 10):\n",
      "  learning_rate = 4.75e-03, loss_average = 2.14e-01\n",
      "  training samples: 1200, AUC : 0.58, loss: 2.1382e-01\n",
      "  validation samples: 3160, AUC : 0.52, loss: 2.1437e-01\n",
      "  time: 747s (wall 133s)\n",
      "step 200 / 1200 (epoch 1.67 / 10):\n",
      "  learning_rate = 4.75e-03, loss_average = nan\n",
      "  training samples: 1200, AUC : 0.58, loss: 2.1424e-01\n",
      "  validation samples: 3160, AUC : 0.48, loss: 2.1447e-01\n",
      "  time: 995s (wall 176s)\n",
      "step 250 / 1200 (epoch 2.08 / 10):\n",
      "  learning_rate = 4.51e-03, loss_average = nan\n",
      "  training samples: 1200, AUC : 0.59, loss: 2.1363e-01\n",
      "  validation samples: 3160, AUC : 0.48, loss: 2.1453e-01\n",
      "  time: 1243s (wall 218s)\n",
      "step 300 / 1200 (epoch 2.50 / 10):\n",
      "  learning_rate = 4.51e-03, loss_average = nan\n",
      "  training samples: 1200, AUC : 0.61, loss: 2.1332e-01\n",
      "  validation samples: 3160, AUC : 0.48, loss: 2.1446e-01\n",
      "  time: 1490s (wall 259s)\n",
      "step 350 / 1200 (epoch 2.92 / 10):\n",
      "  learning_rate = 4.51e-03, loss_average = nan\n",
      "  training samples: 1200, AUC : 0.62, loss: 2.1195e-01\n",
      "  validation samples: 3160, AUC : 0.48, loss: 2.1470e-01\n",
      "  time: 1737s (wall 301s)\n",
      "step 400 / 1200 (epoch 3.33 / 10):\n",
      "  learning_rate = 4.29e-03, loss_average = nan\n",
      "  training samples: 1200, AUC : 0.63, loss: 2.1186e-01\n",
      "  validation samples: 3160, AUC : 0.49, loss: 2.1455e-01\n",
      "  time: 1983s (wall 342s)\n",
      "step 450 / 1200 (epoch 3.75 / 10):\n",
      "  learning_rate = 4.29e-03, loss_average = nan\n",
      "  training samples: 1200, AUC : 0.66, loss: 2.1331e-01\n",
      "  validation samples: 3160, AUC : 0.48, loss: 2.1424e-01\n",
      "  time: 2234s (wall 386s)\n",
      "step 500 / 1200 (epoch 4.17 / 10):\n",
      "  learning_rate = 4.07e-03, loss_average = nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-35e2d32bc62f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels_siamese\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msiamese_cgcnn_cor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time per step: {:.2f} ms'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_step\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Working_Space/fmri/gcn_metric_learning-master/lib/models_siamese.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, train_labels, train_site, val_data, val_labels, val_site)\u001b[0m\n\u001b[1;32m    654\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  learning_rate = {:.2e}, loss_average = {:.2e}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_average\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m                 \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_site\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  training {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Working_Space/fmri/gcn_metric_learning-master/lib/models_siamese.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, data, labels, site, sess)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \"\"\"\n\u001b[1;32m     95\u001b[0m         \u001b[0mt_process\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_wall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Working_Space/fmri/gcn_metric_learning-master/lib/models_siamese.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, labels, site, sess)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mph_site\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_site\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mbatch_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run model\n",
    "site_train = np.ones(all_combs.shape[0], dtype=np.int32)\n",
    "\n",
    "model = models_siamese.siamese_cgcnn_cor(L, **params)\n",
    "accuracy, loss, t_step, scores_summary = model.fit(X_train, y_train, site_train, X_test, y_test, site_test)\n",
    "print('Time per step: {:.2f} ms'.format(t_step*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
